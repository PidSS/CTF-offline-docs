<!DOCTYPE html>
<html ⚡>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">

    <title>LeNet：第一个卷积神经网络</title>

    <meta name="description" content="LeNet 是 Yann LeCun 等人在上世纪 90 年代提出的网络模型，用于手写数字的识别。本文介绍了该模型，并提供 PyTorch 实现。" />
    <link rel="icon" href="../../content/images/size/w256h256/2020/02/small-3.png" type="image/png" />
    <link rel="canonical" href="../index.html" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    
    <meta property="og:site_name" content="Pion1eer" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="LeNet：第一个卷积神经网络" />
    <meta property="og:description" content="LeNet 是 Yann LeCun 等人在上世纪 90 年代提出的网络模型，用于手写数字的识别。本文介绍了该模型，并提供 PyTorch 实现。" />
    <meta property="og:url" content="https://www.ruanx.net/lenet/" />
    <meta property="og:image" content="https://www.ruanx.net/content/images/2022/01/--.jpg" />
    <meta property="article:published_time" content="2021-02-24T07:54:03.000Z" />
    <meta property="article:modified_time" content="2021-02-24T12:37:21.000Z" />
    <meta property="article:tag" content="machine learning" />
    <meta property="article:tag" content="algorithm" />
    
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="LeNet：第一个卷积神经网络" />
    <meta name="twitter:description" content="LeNet 是 Yann LeCun 等人在上世纪 90 年代提出的网络模型，用于手写数字的识别。本文介绍了该模型，并提供 PyTorch 实现。" />
    <meta name="twitter:url" content="https://www.ruanx.net/lenet/" />
    <meta name="twitter:image" content="https://www.ruanx.net/content/images/2022/01/--.jpg" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Ruan Xingzhi" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="machine learning, algorithm" />
    <meta property="og:image:width" content="2000" />
    <meta property="og:image:height" content="1250" />
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Pion1eer",
        "url": "https://www.ruanx.net/",
        "logo": {
            "@type": "ImageObject",
            "url": "https://www.ruanx.net/content/images/size/w256h256/2020/02/small-3.png",
            "width": 60,
            "height": 60
        }
    },
    "author": {
        "@type": "Person",
        "name": "Ruan Xingzhi",
        "image": {
            "@type": "ImageObject",
            "url": "https://www.ruanx.net/content/images/2020/05/blue.jpeg",
            "width": 1024,
            "height": 1024
        },
        "url": "https://www.ruanx.net/author/blue/",
        "sameAs": []
    },
    "headline": "LeNet：第一个卷积神经网络",
    "url": "https://www.ruanx.net/lenet/",
    "datePublished": "2021-02-24T07:54:03.000Z",
    "dateModified": "2021-02-24T12:37:21.000Z",
    "keywords": "machine learning, algorithm",
    "description": "LeNet 是 Yann LeCun 等人在上世纪 90 年代提出的网络模型，用于手写数字的识别。本文介绍了该模型，并提供 PyTorch 实现。",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://www.ruanx.net/"
    }
}
    </script>

    <meta name="generator" content="Ghost 5.8" />
    <link rel="alternate" type="application/rss+xml" title="Pion1eer" href="../../rss/index.rss" />

    <style amp-custom>*,
    *::before,
    *::after {
        box-sizing: border-box;
    }

    html {
        overflow-x: hidden;
        overflow-y: scroll;
        font-size: 62.5%;
        -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
    }

    body {
        min-height: 100vh;
        margin: 0;
        padding: 0;
        color: #3a4145;
        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif;
        font-size: 1.7rem;
        line-height: 1.55em;
        font-weight: 400;
        font-style: normal;
        background: #fff;
        scroll-behavior: smooth;
        overflow-x: hidden;
        -webkit-font-smoothing: antialiased;
        -moz-osx-font-smoothing: grayscale;
    }

    p,
    ul,
    ol,
    li,
    dl,
    dd,
    hr,
    pre,
    form,
    table,
    video,
    figure,
    figcaption,
    blockquote {
        margin: 0;
        padding: 0;
    }

    ul[class],
    ol[class] {
        padding: 0;
        list-style: none;
    }

    img {
        display: block;
        max-width: 100%;
    }

    input,
    button,
    select,
    textarea {
        font: inherit;
        -webkit-appearance: none;
    }

    fieldset {
        margin: 0;
        padding: 0;
        border: 0;
    }

    label {
        display: block;
        font-size: 0.9em;
        font-weight: 700;
    }

    hr {
        position: relative;
        display: block;
        width: 100%;
        height: 1px;
        border: 0;
        border-top: 1px solid currentcolor;
        opacity: 0.1;
    }

    ::selection {
        text-shadow: none;
        background: #cbeafb;
    }

    mark {
        background-color: #fdffb6;
    }

    small {
        font-size: 80%;
    }

    sub,
    sup {
        position: relative;
        font-size: 75%;
        line-height: 0;
        vertical-align: baseline;
    }
    sup {
        top: -0.5em;
    }
    sub {
        bottom: -0.25em;
    }

    ul li + li {
        margin-top: 0.6em;
    }

    a {
        color: var(--ghost-accent-color, #1292EE);
        text-decoration-skip-ink: auto;
    }

    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
        margin: 0;
        font-weight: 700;
        color: #121212;
        line-height: 1.4em;
    }

    h1 {
        font-size: 3.4rem;
        line-height: 1.1em;
    }

    h2 {
        font-size: 2.4rem;
        line-height: 1.2em;
    }

    h3 {
        font-size: 1.8rem;
    }

    h4 {
        font-size: 1.7rem;
    }

    h5 {
        font-size: 1.6rem;
    }

    h6 {
        font-size: 1.6rem;
    }

    amp-img {
        height: 100%;
        width: 100%;
        max-width: 100%;
        max-height: 100%;
    }

    amp-img img {
        object-fit: cover;
    }

    .page-header {
        padding: 50px 5vmin 30px;
        text-align: center;
        font-size: 2rem;
        text-transform: uppercase;
        letter-spacing: 0.5px;
    }

    .page-header a {
        color: #121212;
        font-weight: 700;
        text-decoration: none;
        font-size: 1.6rem;
        letter-spacing: -0.1px;
    }

    .post {
        max-width: 680px;
        margin: 0 auto;
    }

    .post-header {
        margin: 0 5vmin 5vmin;
        text-align: center;
    }

    .post-meta {
        margin: 1rem 0 0 0;
        text-transform: uppercase;
        color: #738a94;
        font-weight: 500;
        font-size: 1.3rem;
    }

    .post-image {
        margin: 0 0 5vmin;
    }

    .post-image img {
        display: block;
        width: 100%;
        height: auto;
    }

    .post-content {
        padding: 0 5vmin;
    }

    .post-content > * + * {
        margin-top: 1.5em;
    }

    .post-content [id]:not(:first-child) {
        margin: 2em 0 0;
    }

    .post-content > [id] + * {
        margin-top: 1rem;
    }

    .post-content [id] + .kg-card,
    .post-content blockquote + .kg-card {
        margin-top: 40px;
    }

    .post-content > ul,
    .post-content > ol,
    .post-content > dl {
        padding-left: 1.9em;
    }

    .post-content hr {
        margin-top: 40px;
    }

    .post .post-content hr + * {
        margin-top: 40px;
    }

    .post-content amp-img {
        background-color: #f8f8f8;
    }

    .post-content blockquote {
        position: relative;
        font-style: italic;
    }

    .post-content blockquote::before {
        content: "";
        position: absolute;
        left: -1.5em;
        top: 0;
        bottom: 0;
        width: 0.3rem;
        background: var(--ghost-accent-color, #1292EE);
    }

    .post-content blockquote.kg-blockquote-alt {
        font-size: 1.2em;
        font-style: italic;
        line-height: 1.6em;
        text-align: center;
        color: #738a94;
        padding: 0.75em 3em 1.25em;
    }

    .post-content blockquote.kg-blockquote-alt::before {
        display: none;
    }

    .post-content :not(.kg-card):not([id]) + .kg-card {
        margin-top: 40px;
    }

    .post-content .kg-card + :not(.kg-card) {
        margin-top: 40px;
    }

    .kg-card figcaption {
        padding: 1.5rem 1.5rem 0;
        text-align: center;
        font-weight: 500;
        font-size: 1.3rem;
        line-height: 1.4em;
        opacity: 0.6;
    }

    .kg-card figcaption strong {
        color: rgba(0,0,0,0.8);
    }

    .post-content :not(pre) code {
        vertical-align: middle;
        padding: 0.15em 0.4em 0.15em;
        border: #e1eaef 1px solid;
        font-weight: 400;
        font-size: 0.9em;
        line-height: 1em;
        color: #15171a;
        background: #f0f6f9;
        border-radius: 0.25em;
    }

    .post-content > pre {
        overflow: scroll;
        padding: 16px 20px;
        color: #fff;
        background: #1F2428;
        border-radius: 5px;
        box-shadow: 0 2px 6px -2px rgba(0,0,0,.1), 0 0 1px rgba(0,0,0,.4);
    }

    .kg-embed-card {
        display: flex;
        flex-direction: column;
        align-items: center;
        width: 100%;
    }

    .kg-image-card img {
        margin: auto;
    }

    .kg-gallery-card + .kg-gallery-card {
        margin-top: 0.75em;
    }

    .kg-gallery-container {
        position: relative;
    }

    .kg-gallery-row {
        display: flex;
        flex-direction: row;
        justify-content: center;
    }

    .kg-gallery-image {
        width: 100%;
        height: 100%;
    }

    .kg-gallery-row:not(:first-of-type) {
        margin: 0.75em 0 0 0;
    }

    .kg-gallery-image:not(:first-of-type) {
        margin: 0 0 0 0.75em;
    }

    .kg-bookmark-card,
    .kg-bookmark-publisher {
        position: relative;
    }

    .kg-bookmark-container,
    .kg-bookmark-container:hover {
        display: flex;
        flex-wrap: wrap;
        flex-direction: row-reverse;
        color: currentColor;
        background: rgba(255,255,255,0.6);
        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif;
        text-decoration: none;
        border-radius: 3px;
        box-shadow: 0 2px 6px -2px rgba(0, 0, 0, 0.1), 0 0 1px rgba(0, 0, 0, 0.4);
        overflow: hidden;
    }

    .kg-bookmark-content {
        flex-basis: 0;
        flex-grow: 999;
        padding: 20px;
        order: 1;
    }

    .kg-bookmark-title {
        font-weight: 600;
        font-size: 1.5rem;
        line-height: 1.3em;
    }

    .kg-bookmark-description {
        display: -webkit-box;
        max-height: 45px;
        margin: 0.5em 0 0 0;
        font-size: 1.4rem;
        line-height: 1.55em;
        overflow: hidden;
        opacity: 0.8;
        -webkit-line-clamp: 2;
        -webkit-box-orient: vertical;
    }

    .kg-bookmark-metadata {
        margin-top: 20px;
    }

    .kg-bookmark-metadata {
        display: flex;
        align-items: center;
        font-weight: 500;
        font-size: 1.3rem;
        line-height: 1.3em;
        white-space: nowrap;
        overflow: hidden;
        text-overflow: ellipsis;
    }

    .kg-bookmark-description {
        display: -webkit-box;
        -webkit-box-orient: vertical;
        -webkit-line-clamp: 2;
        overflow: hidden;
    }

    .kg-bookmark-metadata amp-img {
        width: 18px;
        height: 18px;
        max-width: 18px;
        max-height: 18px;
        margin-right: 10px;
    }

    .kg-bookmark-thumbnail {
        display: flex;
        flex-basis: 20rem;
        flex-grow: 1;
        justify-content: flex-end;
    }

    .kg-bookmark-thumbnail amp-img {
        max-height: 200px;
    }

    .kg-bookmark-author {
        white-space: nowrap;
        text-overflow: ellipsis;
        overflow: hidden;
    }

    .kg-bookmark-publisher::before {
        content: "•";
        margin: 0 .5em;
    }

    .kg-toggle-card-icon {
        display: none;
    }

    .kg-toggle-content {
        margin-top: 0.8rem;
    }

    .kg-product-card-container {
        background: transparent;
        padding: 20px;
        width: 100%;
        border-radius: 5px;
        box-shadow: inset 0 0 0 1px rgb(124 139 154 / 25%);
    }

    .kg-product-card-description p {
        margin-top: 1.5em;
    }

    .kg-product-card-description ul {
        margin-left: 24px;
    }

    .kg-product-card-title {
        font-size: 1.9rem;
        font-weight: 700;
    }

    .kg-product-card-rating-star {
        height: 28px;
        width: 20px;
        margin-right: 2px;
    }

    .kg-product-card-rating-star svg {
    width: 16px;
    height: 16px;
    fill: currentColor;
    opacity: 0.15;
    }

    .kg-product-card-rating-active.kg-product-card-rating-star svg {
    opacity: 1;
    }

    .kg-nft-card-container {
        position: relative;
        display: flex;
        flex: auto;
        flex-direction: column;
        text-decoration: none;
        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif;
        font-size: 1.4rem;
        font-weight: 400;
        box-shadow: 0 2px 6px -2px rgb(0 0 0 / 10%), 0 0 1px rgb(0 0 0 / 40%);
        width: 100%;
        max-width: 512px;
        color: #15212A;
        background: #fff;
        border-radius: 5px;
        transition: none;
        margin: 0 auto;
    }

    .kg-nft-metadata {
        padding: 2.0rem;
    }

    .kg-nft-image-container {
        position: relative;
    }

    .kg-nft-image {
        display: flex;
        border-radius: 5px 5px 0 0;
    }

    .kg-nft-header {
        display: flex;
        justify-content: space-between;
        align-items: flex-start;
        gap: 20px;
    }

    .kg-nft-header h4.kg-nft-title {
        font-size: 1.9rem;
        font-weight: 700;
        margin: 0;
        color: #15212A;
    }

    .kg-nft-header amp-img {
        max-width: 114px;
        max-height: 26px;
    }

    .kg-nft-opensea-logo {
        margin-top: 2px;
        width: 100px;
    }

    .kg-nft-creator {
        font-family: inherit;
        color: #95A1AD;
    }

    .kg-nft-creator span {
        font-weight: 500;
        color: #15212A;
    }

    .kg-nft-card p.kg-nft-description {
        font-size: 1.4rem;
        line-height: 1.4em;
        margin: 2.0rem 0 0;
        color: #222;
    }

    .kg-button-card {
        display: flex;
        position: static;
        align-items: center;
        width: 100%;
        justify-content: center;
    }

    .kg-btn {
        display: flex;
        position: static;
        align-items: center;
        padding: 0 2.0rem;
        height: 4.0rem;
        line-height: 4.0rem;
        font-size: 1.65rem;
        font-weight: 600;
        text-decoration: none;
        border-radius: 5px;
        transition: opacity 0.2s ease-in-out;
    }

    .kg-btn:hover {
        opacity: 0.85;
    }

    .kg-btn-accent {
        background-color: var(--ghost-accent-color, #1292EE);
        color: #fff;
    }

    .kg-callout-card {
        display: flex;
        padding: 20px 28px;
        border-radius: 3px;
    }

    .kg-callout-card-grey {
        background: rgba(124, 139, 154, 0.13);
    }

    .kg-callout-card-white {
        background: transparent;
        box-shadow: inset 0 0 0 1px rgba(124, 139, 154, 0.25);
    }

    .kg-callout-card-blue {
        background: rgba(33, 172, 232, 0.12);
    }

    .kg-callout-card-green {
        background: rgba(52, 183, 67, 0.12);
    }

    .kg-callout-card-yellow {
        background: rgba(240, 165, 15, 0.13);
    }

    .kg-callout-card-red {
        background: rgba(209, 46, 46, 0.11);
    }

    .kg-callout-card-pink {
        background: rgba(225, 71, 174, 0.11);
    }

    .kg-callout-card-purple {
        background: rgba(135, 85, 236, 0.12);
    }

    .kg-callout-card-accent {
        background: var(--ghost-accent-color);
        color: #fff;
    }

    .kg-callout-card-accent a {
        color: #fff;
    }

    .kg-callout-emoji {
        padding-right: 16px;
        line-height: 1.3;
        font-size: 1.25em;
    }

    .kg-header-card {
        padding: 6em 3em;
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: center;
        text-align: center;
    }

    .kg-header-card.kg-size-small {
        padding-top: 4em;
        padding-bottom: 4em;
    }

    .kg-header-card.kg-size-large {
        padding-top: 12em;
        padding-bottom: 12em;
    }

    .kg-header-card.kg-width-full {
        padding-left: 4em;
        padding-right: 4em;
    }

    .kg-header-card.kg-align-left {
        text-align: left;
        align-items: flex-start;
    }

    .kg-header-card.kg-style-dark {
        background: #15171a;
        color: #ffffff;
    }

    .kg-header-card.kg-style-light {
        color: #15171a;
        border: 1px solid rgba(124, 139, 154, 0.25);
        border-width: 1px 0;
    }

    .kg-header-card.kg-style-accent {
        background-color: var(--ghost-accent-color);
    }

    .kg-header-card.kg-style-image {
        background-color: #e7e7eb;
        background-size: cover;
        background-position: center center;
    }

    .kg-header-card h2 {
        font-size: 4em;
        font-weight: 700;
        line-height: 1.1em;
        margin: 0;
    }

    .kg-header-card h2 strong {
        font-weight: 800;
    }

    .kg-header-card.kg-size-small h2 {
        font-size: 3em;
    }

    .kg-header-card.kg-size-large h2 {
        font-size: 5em;
    }

    .kg-header-card h3 {
        font-size: 1.25em;
        font-weight: 500;
        line-height: 1.3em;
        margin: 0;
    }

    .kg-header-card h3 strong {
        font-weight: 600;
    }

    .kg-header-card.kg-size-small h3 {
        font-size: 1em;
    }

    .kg-header-card.kg-size-large h3 {
        font-size: 1.5em;
    }

    .kg-header-card:not(.kg-style-light) h2,
    .kg-header-card:not(.kg-style-light) h3 {
        color: #ffffff;
    }

    .kg-header-card a.kg-header-card-button {
        display: flex;
        position: static;
        align-items: center;
        padding: 0 1.2em;
        height: 2.4em;
        line-height: 1em;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Oxygen", "Ubuntu", "Cantarell", "Fira Sans", "Droid Sans", "Helvetica Neue", sans-serif;
        font-size: 0.95em;
        font-weight: 600;
        text-decoration: none;
        border-radius: 5px;
        transition: opacity 0.2s ease-in-out;
        background-color: var(--ghost-accent-color);
        color: #ffffff;
        margin: 1.75em 0 0;
    }

    .kg-header-card a.kg-header-card-button:hover {
        opacity: 0.85;
    }

    .kg-header-card.kg-size-large a.kg-header-card-button {
        margin-top: 2em;
    }

    .kg-header-card.kg-size-small a.kg-header-card-button {
        margin-top: 1.5em;
    }

    .kg-header-card.kg-style-image a.kg-header-card-button,
    .kg-header-card.kg-style-dark a.kg-header-card-button {
        background: #ffffff;
        color: #15171a;
    }

    .kg-header-card.kg-style-accent a.kg-header-card-button {
        background: #ffffff;
        color: var(--ghost-accent-color);
    }

    .kg-audio-card {
        display: flex;
        width: 100%;
        box-shadow: inset 0 0 0 1px rgba(124, 139, 154, 0.25);
    }

    .kg-audio-thumbnail {
        display: flex;
        justify-content: center;
        align-items: center;
        width: 80px;
        min-width: 80px;
        height: 80px;
        background: transparent;
        object-fit: cover;
        aspect-ratio: 1/1;
        border-radius: 3px 0 0 3px;
    }

    .kg-audio-thumbnail.placeholder {
        background: var(--ghost-accent-color);
    }

    .kg-audio-thumbnail.placeholder svg {
        width: 24px;
        height: 24px;
        fill: white;
    }

    .kg-audio-player-container {
        position: relative;
        display: flex;
        flex-direction: column;
        justify-content: space-between;
        width: 100%;
        --seek-before-width: 0%;
        --volume-before-width: 100%;
        --buffered-width: 0%;
    }

    .kg-audio-title {
        width: 100%;
        padding: 8px 12px 0;
        border: none;
        font-family: inherit;
        font-size: 1.1em;
        font-weight: 700;
        background: transparent;
    }

    .kg-audio-player {
        display: none;
    }

    .kg-width-full.kg-card-hascaption {
        display: grid;
        grid-template-columns: inherit;
    }

    .post-content table {
        border-collapse: collapse;
        width: 100%;
    }

    .post-content th {
        padding: 0.5em 0.8em;
        text-align: left;
        font-size: .75em;
        text-transform: uppercase;
    }

    .post-content td {
        padding: 0.4em 0.7em;
    }

    .post-content tbody tr:nth-child(2n + 1) {
        background-color: rgba(0,0,0,0.1);
        padding: 1px;
    }

    .post-content tbody tr:nth-child(2n + 2) td:last-child {
        box-shadow:
            inset 1px 0 rgba(0,0,0,0.1),
            inset -1px 0 rgba(0,0,0,0.1);
    }

    .post-content tbody tr:nth-child(2n + 2) td {
        box-shadow: inset 1px 0 rgba(0,0,0,0.1);
    }

    .post-content tbody tr:last-child {
        border-bottom: 1px solid rgba(0,0,0,.1);
    }

    .page-footer {
        padding: 60px 5vmin;
        margin: 60px auto 0;
        text-align: center;
        background-color: #f8f8f8;
    }

    .page-footer h3 {
        margin: 0.5rem 0 0 0;
    }

    .page-footer p {
        max-width: 500px;
        margin: 1rem auto 1.5rem;
        font-size: 1.7rem;
        line-height: 1.5em;
        color: rgba(0,0,0,0.6)
    }

    .powered {
        display: inline-flex;
        align-items: center;
        margin: 30px 0 0;
        padding: 6px 9px 6px 6px;
        border: rgba(0,0,0,0.1) 1px solid;
        font-size: 12px;
        line-height: 12px;
        letter-spacing: -0.2px;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Oxygen", "Ubuntu", "Cantarell", "Fira Sans", "Droid Sans", "Helvetica Neue", sans-serif;
        font-weight: 500;
        color: #222;
        text-decoration: none;
        background: #fff;
        border-radius: 6px;
    }

    .powered svg {
        height: 16px;
        width: 16px;
        margin: 0 6px 0 0;
    }

    @media (max-width: 600px) {
        body {
            font-size: 1.6rem;
        }
        h1 {
            font-size: 3rem;
        }

        h2 {
            font-size: 2.2rem;
        }
    }

    @media (max-width: 400px) {
        h1 {
            font-size: 2.6rem;
            line-height: 1.15em;
        }
        h2 {
            font-size: 2rem;
            line-height: 1.2em;
        }
        h3 {
            font-size: 1.7rem;
        }
    }

    :root {--ghost-accent-color: #15171A;}
    </style>

    <style amp-boilerplate>body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-moz-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript><style amp-boilerplate>body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript>
    <script async src="https://cdn.ampproject.org/v0.js"></script>

    

</head>

<body class="amp-template">
    <header class="page-header">
        <a href="../../index.html">
                <amp-img class="site-icon" src="https://www.ruanx.net/content/images/2020/02/small-3.png" width="50" height="50" layout="fixed" alt="Pion1eer"></amp-img>
        </a>
    </header>

    <main class="content" role="main">
        <article class="post">

            <header class="post-header">
                <h1 class="post-title">LeNet：第一个卷积神经网络</h1>
                <section class="post-meta">
                    Ruan Xingzhi -
                    <time class="post-date" datetime="2021-02-24">24 Feb 2021</time>
                </section>
            </header>
            <section class="post-content">

                <p>　　「识别手写数字」是一个经典的机器学习任务，有著名的 MNIST 数据集。我们曾经利用多层感知机实现了 90+% 的准确率，本文将介绍卷积神经网络 LeNet，主要参考 <a href="https://towardsdatascience.com/understanding-lenet-a-detailed-walkthrough-17833d4bd155">这篇英文博客</a> 的讲解。</p><p>　　LeNet 是几种神经网络的统称，它们是 Yann LeCun 等人在 1990 年代开发的。一般认为，它们是最早的<strong>卷积神经网络</strong>（Convolutional Neural Networks, CNNs）。模型接收灰度图像，并输出其中包含的手写数字。LeNet 包含了以下三个模型：</p><ul><li>LeNet-1：5 层模型，一个简单的 CNN。</li><li>LeNet-4：6 层模型，是 LeNet-1 的改进版本。</li><li>LeNet-5：7 层模型，最著名的版本。</li></ul><p>　　CNN 的设计是为了模拟人眼的感知方式。传统的 CNN 一般包含以下三种层：</p><ul><li>卷积层（convolution layers）</li><li>降采样层（subsampling layers），或称池化层（pooling layers）</li><li>全连接层（fully connected layers）</li></ul><p>　　它们有各自的用途，通过排列这些层，我们能实现各种 CNN。接下来，我们分别介绍这三种部件。</p><h3 id="-">卷积层</h3><p>　　卷积层利用卷积核（kernel，或称 filter）对图像进行卷积运算。每个 kernel 都是可训练的，有些 kernel 还有 bias 参数。执行卷积运算时，把 kernel 在原图上移动，步长为指定的参数 stride，有时还会做 padding。对于原图内容，将其与 kernel 先进行元素乘法，把结果求和，再加上 bias，就得到输出。</p><p>　　把原图跑完一遍之后，我们在各个位置的卷积结果，生成了特征图（feature map）。显然，kernel 越大，步长越长，padding 越小，则特征图越小。具体而言，满足：$$\text{output size} =\frac{\text{input size} - \text{kernel size} + 2\times \text{padding}}{\text{stride}} + 1$$</p><figure class="kg-card kg-image-card kg-card-hascaption"><amp-img src="https://www.ruanx.net/content/images/2021/02/1.png" class="kg-image" alt width="1152" height="576" srcset="https://www.ruanx.net/content/images/size/w600/2021/02/1.png 600w, https://www.ruanx.net/content/images/size/w1000/2021/02/1.png 1000w, https://www.ruanx.net/content/images/2021/02/1.png 1152w" layout="responsive"></amp-img><figcaption>卷积运算。原图和 kernel 均有 3 个通道。图片来源：<a href="https://towardsdatascience.com/understanding-lenet-a-detailed-walkthrough-17833d4bd155">Azel Daniel</a>，下同。</figcaption></figure><p>　　上图展示了一个典型的二维卷积。原图是 3 通道的，不填充，步长为 0。最终将生成 4×4 的 3 通道特征图。</p><h3 id="--1">池化层</h3><p>　　池化层一般是不用训练的。它的目的是对特征进行降取样，显著减少特征的个数来方便学习。一般有两种池化方式：均值池化（average pooling）和最大池化（max pooling）。前者计算出一个区域的均值，后者从区域中取最大值。</p><figure class="kg-card kg-image-card kg-card-hascaption"><amp-img src="https://www.ruanx.net/content/images/2021/02/2.png" class="kg-image" alt width="1152" height="576" srcset="https://www.ruanx.net/content/images/size/w600/2021/02/2.png 600w, https://www.ruanx.net/content/images/size/w1000/2021/02/2.png 1000w, https://www.ruanx.net/content/images/2021/02/2.png 1152w" layout="responsive"></amp-img><figcaption>池化过程。</figcaption></figure><p>　　上图展示了一个不填充、步长为 2 的池化。6×6×3 的原图在池化之后，变成了 3×3×3 的大小。</p><h3 id="--2">全连接层</h3><p>　　全连接层一般用于 CNN 的最后几层，负责提取卷积和池化之后的特征。这里不过多介绍。接下来，我们开始讨论几种 LeNet。</p><hr></hr><h3 id="lenet-1">LeNet-1</h3><p>　　LeNet-1 仅有五个层，结构如下：</p><figure class="kg-card kg-image-card"><amp-img src="https://www.ruanx.net/content/images/2021/02/image-5.png" class="kg-image" alt width="1728" height="864" srcset="https://www.ruanx.net/content/images/size/w600/2021/02/image-5.png 600w, https://www.ruanx.net/content/images/size/w1000/2021/02/image-5.png 1000w, https://www.ruanx.net/content/images/size/w1600/2021/02/image-5.png 1600w, https://www.ruanx.net/content/images/2021/02/image-5.png 1728w" layout="responsive"></amp-img></figure><p>　　原先的 28×28 的灰度图像，先经过一个卷积层，生成 4 通道的 feature map；再进行池化降维，然后通过卷积层，得到 12 通道的 feature map。再次池化之后，用全连接层提取特征。具体参数如下：</p><ul><li>C1：卷积层，num_kernels=4, kernel_size=5×5, padding=0, stride=1</li><li>S2：均值池化层，kernel_size=2×2, padding=0, stride=2</li><li>C3：卷积层，num_kernels=12, kernel_size=5×5, padding=0, stride=1</li><li>S4：均值池化层，kernel_size=2×2, padding=0, stride=2</li><li>F5：全连接层，out_features=10</li></ul><p>　　我们需要解释一下 C3 如何用 12 个 kernel，从 4 通道的图中提取出 12 个通道的特征。C3 的每个输出层，是与指定的一些 S2 层相连的，对应的 kernel 长宽是 5×5×n，其中 n 是它所连接的 S2 层数。连接分组方式是超参数（当然我们经常懒得去搞这件事，往往令每个输出层与所有输入层相连，下面的代码也是这样处理的）。</p><figure class="kg-card kg-code-card"><pre><code class="language-python">import torch
import torchvision
import torch.nn as nn
import torchvision.transforms as transforms
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt

trans_to_tensor = transforms.Compose([
    transforms.ToTensor()
])

data_train = torchvision.datasets.MNIST(
    './data', 
    train=True, 
    transform=trans_to_tensor, 
    download=True)

data_test = torchvision.datasets.MNIST(
    './data', 
    train=False, 
    transform=trans_to_tensor, 
    download=True)

data_train, data_test

'''
(Dataset MNIST
     Number of datapoints: 60000
     Root location: ./data
     Split: Train
     StandardTransform
 Transform: Compose(
                ToTensor()
            ),
 Dataset MNIST
     Number of datapoints: 10000
     Root location: ./data
     Split: Test
     StandardTransform
 Transform: Compose(
                ToTensor()
            ))
'''</code></pre><figcaption>▲ 加载数据</figcaption></figure><p>　　我们随便展示一个数据：</p><pre><code class="language-python">train_loader = torch.utils.data.DataLoader(data_train, batch_size=100, shuffle=True)

x, y = next(iter(train_loader))

plt.imshow(x[0].squeeze(0), cmap='gray'), y[0]</code></pre><figure class="kg-card kg-image-card"><amp-img src="https://www.ruanx.net/content/images/2021/02/image-6.png" class="kg-image" alt width="897" height="355" srcset="https://www.ruanx.net/content/images/size/w600/2021/02/image-6.png 600w, https://www.ruanx.net/content/images/2021/02/image-6.png 897w" layout="responsive"></amp-img></figure><p>　　训练网络：</p><pre><code class="language-python">def test(net):
    net.eval()
    
    test_loader = torch.utils.data.DataLoader(data_train, batch_size=10000, shuffle=False)
    test_data = next(iter(test_loader))
    
    with torch.no_grad():
        x, y = test_data[0], test_data[1]
    
        outputs = net(x)

        pred = torch.max(outputs, 1)[1]
        print(f'test acc: {sum(pred == y) / outputs.shape[0]}')
    
    net.train()
    
def fit(net, epoch=1):
    net.train()
    run_loss = 0
    
    for num_epoch in range(epoch):
        print(f'epoch {num_epoch}')
        
        for i, data in enumerate(train_loader):
            x, y = data[0], data[1]

            outputs = net(x)
            loss = criterion(outputs, y)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            run_loss += loss.item()

            if i % 100 == 99:
                print(f'[{(i+1) * 100} / 60000] loss={run_loss / 100}')
                run_loss = 0
                
                test(net)</code></pre><p>　　上面是训练 MNIST 的通用框架。接下来我们实现 LeNet-1：</p><pre><code class="language-python">class LeNet1(nn.Module):
    
    def __init__(self):
        super().__init__()
        
        self.conv1 = nn.Conv2d(1, 4, [5, 5])
        self.pool1 = nn.AvgPool2d([2, 2])
        self.conv2 = nn.Conv2d(4, 12, [5, 5])
        self.pool2 = nn.AvgPool2d([2, 2])
        self.fc1 = nn.Linear(12 * 4 * 4, 10)
    
    def forward(self, x):
        x = torch.tanh(self.conv1(x))
        x = self.pool1(x)
        x = torch.tanh(self.conv2(x))
        x = self.pool2(x)
        x = x.view(-1, 12 * 4 * 4)
        x = self.fc1(x)
        
        return x

net_1 = LeNet1()

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(net_1.parameters())

fit(net_1, epoch=5)

'''
epoch 0
[10000 / 60000] loss=1.3996468645334244
test acc: 0.8370000123977661
[20000 / 60000] loss=0.525577874481678
test acc: 0.8773999810218811
[30000 / 60000] loss=0.40644764140248296
test acc: 0.8932999968528748
[40000 / 60000] loss=0.367893455773592
test acc: 0.9024999737739563
[50000 / 60000] loss=0.3431669014692307
test acc: 0.9133999943733215
[60000 / 60000] loss=0.30854775562882425
test acc: 0.916100025177002

....

epoch 4
[10000 / 60000] loss=0.1185341552272439
test acc: 0.9692000150680542
[20000 / 60000] loss=0.10541347645223141
test acc: 0.968999981880188
[30000 / 60000] loss=0.107633958440274
test acc: 0.9714000225067139
[40000 / 60000] loss=0.10533566184341908
test acc: 0.9721999764442444
[50000 / 60000] loss=0.10117398623377084
test acc: 0.9725000262260437
[60000 / 60000] loss=0.09958926556631922
test acc: 0.9736999869346619
'''</code></pre><p>　　可见 LeNet-1 的准确度还是很高的，训练 20 个 epoch 之后可以达到 98.90% 的准确率。另外，我没有调超参数，调一调之后还可以涨更多的点，Github 上有 <a href="https://github.com/HangJie720/lenet-1/blob/master/Lenet.ipynb">网友的代码</a> 的 batchsize 是 64，拿了99.30% 准确率。从 Adam 换成 SGD 也许可以再涨一些点。</p><p>　　接下来要问：卷积起到了什么作用？我们知道，卷积可以完成很多操作，例如高斯模糊、边缘提取等，这是因为卷积是对原图中一块相邻区域的运算。传统的多层感知机，对像素位置之间的关联是很弱的，而卷积层输出的 feature map 中，每个值都是由原图中的一个块产生，故考虑了相邻像素之间的关系。</p><p>　　下面展示两张图片经过 conv1 层之后的结果：</p><figure class="kg-card kg-image-card kg-width-wide"><amp-img src="https://www.ruanx.net/content/images/2021/02/3-1.png" class="kg-image" alt width="1150" height="477" srcset="https://www.ruanx.net/content/images/size/w600/2021/02/3-1.png 600w, https://www.ruanx.net/content/images/size/w1000/2021/02/3-1.png 1000w, https://www.ruanx.net/content/images/2021/02/3-1.png 1150w" layout="responsive"></amp-img></figure><p>　　可以发现，第一个 kernel 可以很好地提取「左上－右下」的线条；第二、三个 kernel 可以提取横向的线条；第四个 kernel 可以提取「左下－右上」的线条。这不是我们钦定的，而是它们自己学习出来的。我们从中可以体会到 CNN 提取图片「部件」的独到之处。</p><h3 id="lenet-4">LeNet-4</h3><p>　　LeNet-4 是 LeNet-1 的改进版本，有 6 个层。它们主要的不同之处在于，LeNet-4 采用了两个全连接层来提取特征。另外，LeNet-4 的输入是 32×32 的灰度图。</p><figure class="kg-card kg-image-card"><amp-img src="https://www.ruanx.net/content/images/2021/02/image-7.png" class="kg-image" alt width="1587" height="766" srcset="https://www.ruanx.net/content/images/size/w600/2021/02/image-7.png 600w, https://www.ruanx.net/content/images/size/w1000/2021/02/image-7.png 1000w, https://www.ruanx.net/content/images/2021/02/image-7.png 1587w" layout="responsive"></amp-img></figure><p>　　每个层的参数如下：</p><ul><li>C1：卷积层，num_kernels=4, kernel_size=5×5, padding=0, stride=1</li><li>S2：均值池化层，kernel_size=2×2, padding=0, stride=2</li><li>C3：卷积层，num_kernels=16, kernel_size=5×5, padding=0, stride=1</li><li>S4：均值池化层，kernel_size=2×2, padding=0, stride=2</li><li>F5：全连接层，out_features=120</li><li>F6：全连接层，out_features=10</li></ul><p>　　Yann LeCun 在 USPS 数据集上，测得 LeNet-1 的误判率是 1.7%，有 3246 个参数；LeNet-4 在 MNIST 上的误判率是 1.1%，但有 51050 个参数。像这种架构的 CNN，全连接层占了绝大部分的参数。</p><h3 id="lenet-5">LeNet-5</h3><p>　　LeNet-5 是 LeNet-4 的改进版，即现在我们熟知的那个 LeNet。它的层数达到了 7 层，获取 32×32 的输入，有 60850 个参数。</p><figure class="kg-card kg-image-card"><amp-img src="https://www.ruanx.net/content/images/2021/02/image-8.png" class="kg-image" alt width="1728" height="864" srcset="https://www.ruanx.net/content/images/size/w600/2021/02/image-8.png 600w, https://www.ruanx.net/content/images/size/w1000/2021/02/image-8.png 1000w, https://www.ruanx.net/content/images/size/w1600/2021/02/image-8.png 1600w, https://www.ruanx.net/content/images/2021/02/image-8.png 1728w" layout="responsive"></amp-img></figure><p>　　在 MNIST 数据集上，Yann LeCun 的 LeNet-5 具有 0.95% 的低误判率。其各层参数如下：</p><ul><li>C1：卷积层，num_kernels=6, kernel_size=5×5, padding=0, stride=1</li><li>S2：均值池化层，kernel_size=2×2, padding=0, stride=2</li><li>C3：卷积层，num_kernels=16, kernel_size=5×5, padding=0, stride=1</li><li>S4：均值池化层，kernel_size=2×2, padding=0, stride=2</li><li>F5：全连接层，out_features=140</li><li>F6：全连接层，out_features=84</li><li>F7：全连接层，out_features=10</li></ul><p>　　S2 层有个需要注意的地方：它的每一个输出通道，是由对应的输入通道的池化结果，乘以 weight 再加上 bias。它们都是可训练的，于是 S2 层有了 12 个可训练参数（共 6 个通道，每个通道训练自己的 weight 和 bias）。S4 层与之类似，有 32 个可训练参数。</p><p>　　C3 的每个输出通道，只与 S2 指定的若干个通道相连。具体的分组方法如下：</p><figure class="kg-card kg-image-card kg-card-hascaption"><amp-img src="https://www.ruanx.net/content/images/2021/02/image-9.png" class="kg-image" alt width="570" height="201" layout="responsive"></amp-img><figcaption>每一列是一个输出通道，有「X」的行表示与之相连的 S2 通道</figcaption></figure><p>　　我们在代码里面继续偷懒，让 C3 的每个输出通道与 S2 所有通道相连。另外，由于 LeNet-5 输入为 32×32，而我们手上的 MNIST 是 28×28 的，于是做 2 像素的 padding，来形成 32×32 的图。</p><pre><code class="language-python">class LeNet5(nn.Module):
    
    def __init__(self):
        super().__init__()
        
        self.conv1 = nn.Conv2d(1, 6, 5, padding=2)
        self.pool1 = nn.AvgPool2d(2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.pool2 = nn.AvgPool2d(2)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = torch.tanh(self.conv1(x))
        x = self.pool1(x)
        x = torch.tanh(self.conv2(x))
        x = self.pool2(x)
        
        x = x.view(-1, 16 * 5 * 5)
        x = self.fc1(x)
        x = self.fc2(x)
        x = self.fc3(x)
        
        return x

net_5 = LeNet5()

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(net_5.parameters())

fit(net_5, epoch=20)

'''
epoch 0
[10000 / 60000] loss=0.8946414443850518
test acc: 0.8745999932289124
[20000 / 60000] loss=0.3811839409172535
test acc: 0.9031000137329102
[30000 / 60000] loss=0.34581568479537966
test acc: 0.9114999771118164
[40000 / 60000] loss=0.299122948423028
test acc: 0.9218000173568726
[50000 / 60000] loss=0.25059412196278574
test acc: 0.927299976348877
[60000 / 60000] loss=0.22540301881730557
test acc: 0.9373000264167786

...

epoch 19
[10000 / 60000] loss=0.024130959606263786
test acc: 0.9918000102043152
[20000 / 60000] loss=0.02669174194626976
test acc: 0.9901000261306763
[30000 / 60000] loss=0.02360637279227376
test acc: 0.9883000254631042
[40000 / 60000] loss=0.023022194614750333
test acc: 0.9933000206947327
[50000 / 60000] loss=0.035112172679509966
test acc: 0.9896000027656555
[60000 / 60000] loss=0.03023288542899536
test acc: 0.9898999929428101
'''</code></pre><p>　　注意到 LeNet-5 的泛化能力确实比 LeNet-1 好了一些，这边迭代 20 次之后，可以涨到 99.33% 的准确率。</p><p>　　来看 conv1 处理之后的结果：</p><figure class="kg-card kg-image-card kg-width-wide"><amp-img src="https://www.ruanx.net/content/images/2021/02/4.png" class="kg-image" alt width="1150" height="391" srcset="https://www.ruanx.net/content/images/size/w600/2021/02/4.png 600w, https://www.ruanx.net/content/images/size/w1000/2021/02/4.png 1000w, https://www.ruanx.net/content/images/2021/02/4.png 1150w" layout="responsive"></amp-img></figure><p>　　这里我们可以看出，kernel 3、5 可以提取上边缘，kernel 4 可以提取下边缘。kernel 1 可以提取竖线，kernel 2 可以提取横线。</p><hr></hr><p>　　Yann LeCun 训练这些 LeNet 的时候，对数据进行了预处理，使之均值为 0，方差大概为 1。另外，当时采用的是 MSE loss，本文采用了交叉熵。LeNet 是一个划时代的模型，但它也有一些不足之处：</p><ol><li>网络很小，限制了应用场景。</li><li>LeNet 采用了均值池化，但我们现在倾向于采用最大池化，这样可以加速收敛。</li><li>LeNet 采用的激活函数是 tanh，但我们现在倾向于使用 ReLU。实践上，ReLU 往往能够有更好的正确率。</li></ol><p>　　总结一句，LeNet 给我们提供的思路是：用卷积提取与位置相关的信息；用池化来减少特征数量；用全连接来提取特征、进行预测。</p>

            </section>

        </article>
    </main>
    <footer class="page-footer">
            <amp-img class="site-icon" src="https://www.ruanx.net/content/images/2020/02/small-3.png" width="50" height="50" layout="fixed" alt="Pion1eer"></amp-img>
        <h3>Pion1eer</h3>
            <p>Stand with Ukraine 💙💛</p>
        <p><a href="../../index.html">Read more posts →</a></p>
        <a class="powered" href="https://ghost.org" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 156 156"><g fill="none" fill-rule="evenodd"><rect fill="#15212B" width="156" height="156" rx="27"/><g transform="translate(36 36)" fill="#F6F8FA"><path d="M0 71.007A4.004 4.004 0 014 67h26a4 4 0 014 4.007v8.986A4.004 4.004 0 0130 84H4a4 4 0 01-4-4.007v-8.986zM50 71.007A4.004 4.004 0 0154 67h26a4 4 0 014 4.007v8.986A4.004 4.004 0 0180 84H54a4 4 0 01-4-4.007v-8.986z"/><rect y="34" width="84" height="17" rx="4"/><path d="M0 4.007A4.007 4.007 0 014.007 0h41.986A4.003 4.003 0 0150 4.007v8.986A4.007 4.007 0 0145.993 17H4.007A4.003 4.003 0 010 12.993V4.007z"/><rect x="67" width="17" height="17" rx="4"/></g></g></svg> Published with Ghost</a>
    </footer>
    
</body>
</html>
