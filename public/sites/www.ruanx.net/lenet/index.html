<!DOCTYPE html>
<html lang="en">
<head>

    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <title>LeNet：第一个卷积神经网络</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="stylesheet" type="text/css" href="../assets/built/screen%EF%B9%96v=792c672b3c.css" />

    <meta name="description" content="LeNet 是 Yann LeCun 等人在上世纪 90 年代提出的网络模型，用于手写数字的识别。本文介绍了该模型，并提供 PyTorch 实现。" />
    <link rel="icon" href="../content/images/size/w256h256/2020/02/small-3.png" type="image/png" />
    <link rel="canonical" href="index.html" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    <link rel="amphtml" href="amp/index.html" />
    
    <meta property="og:site_name" content="Pion1eer" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="LeNet：第一个卷积神经网络" />
    <meta property="og:description" content="LeNet 是 Yann LeCun 等人在上世纪 90 年代提出的网络模型，用于手写数字的识别。本文介绍了该模型，并提供 PyTorch 实现。" />
    <meta property="og:url" content="https://www.ruanx.net/lenet/" />
    <meta property="og:image" content="https://www.ruanx.net/content/images/2022/01/--.jpg" />
    <meta property="article:published_time" content="2021-02-24T07:54:03.000Z" />
    <meta property="article:modified_time" content="2021-02-24T12:37:21.000Z" />
    <meta property="article:tag" content="machine learning" />
    <meta property="article:tag" content="algorithm" />
    
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="LeNet：第一个卷积神经网络" />
    <meta name="twitter:description" content="LeNet 是 Yann LeCun 等人在上世纪 90 年代提出的网络模型，用于手写数字的识别。本文介绍了该模型，并提供 PyTorch 实现。" />
    <meta name="twitter:url" content="https://www.ruanx.net/lenet/" />
    <meta name="twitter:image" content="https://www.ruanx.net/content/images/2022/01/--.jpg" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Ruan Xingzhi" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="machine learning, algorithm" />
    <meta property="og:image:width" content="2000" />
    <meta property="og:image:height" content="1250" />
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Pion1eer",
        "url": "https://www.ruanx.net/",
        "logo": {
            "@type": "ImageObject",
            "url": "https://www.ruanx.net/content/images/size/w256h256/2020/02/small-3.png",
            "width": 60,
            "height": 60
        }
    },
    "author": {
        "@type": "Person",
        "name": "Ruan Xingzhi",
        "image": {
            "@type": "ImageObject",
            "url": "https://www.ruanx.net/content/images/2020/05/blue.jpeg",
            "width": 1024,
            "height": 1024
        },
        "url": "https://www.ruanx.net/author/blue/",
        "sameAs": []
    },
    "headline": "LeNet：第一个卷积神经网络",
    "url": "https://www.ruanx.net/lenet/",
    "datePublished": "2021-02-24T07:54:03.000Z",
    "dateModified": "2021-02-24T12:37:21.000Z",
    "keywords": "machine learning, algorithm",
    "description": "LeNet 是 Yann LeCun 等人在上世纪 90 年代提出的网络模型，用于手写数字的识别。本文介绍了该模型，并提供 PyTorch 实现。",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://www.ruanx.net/"
    }
}
    </script>

    <meta name="generator" content="Ghost 5.8" />
    <link rel="alternate" type="application/rss+xml" title="Pion1eer" href="../rss/index.rss" />
    <script defer src="https://cdn.jsdelivr.net/npm/@tryghost/portal@~2.5/umd/portal.min.js" data-ghost="https://www.ruanx.net/" data-key="595acd8f13c14d79a10527399d" data-api="https://www.ruanx.net/ghost/api/content/" crossorigin="anonymous"></script><style id="gh-members-styles">.gh-post-upgrade-cta-content,
.gh-post-upgrade-cta {
    display: flex;
    flex-direction: column;
    align-items: center;
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
    text-align: center;
    width: 100%;
    color: #ffffff;
    font-size: 16px;
}

.gh-post-upgrade-cta-content {
    border-radius: 8px;
    padding: 40px 4vw;
}

.gh-post-upgrade-cta h2 {
    color: #ffffff;
    font-size: 28px;
    letter-spacing: -0.2px;
    margin: 0;
    padding: 0;
}

.gh-post-upgrade-cta p {
    margin: 20px 0 0;
    padding: 0;
}

.gh-post-upgrade-cta small {
    font-size: 16px;
    letter-spacing: -0.2px;
}

.gh-post-upgrade-cta a {
    color: #ffffff;
    cursor: pointer;
    font-weight: 500;
    box-shadow: none;
    text-decoration: underline;
}

.gh-post-upgrade-cta a:hover {
    color: #ffffff;
    opacity: 0.8;
    box-shadow: none;
    text-decoration: underline;
}

.gh-post-upgrade-cta a.gh-btn {
    display: block;
    background: #ffffff;
    text-decoration: none;
    margin: 28px 0 0;
    padding: 8px 18px;
    border-radius: 4px;
    font-size: 16px;
    font-weight: 600;
}

.gh-post-upgrade-cta a.gh-btn:hover {
    opacity: 0.92;
}</style>
    <script defer src="https://cdn.jsdelivr.net/npm/@tryghost/sodo-search@~1.1/umd/sodo-search.min.js" data-key="595acd8f13c14d79a10527399d" data-styles="https://cdn.jsdelivr.net/npm/@tryghost/sodo-search@~1.1/umd/main.css" data-sodo-search="https://www.ruanx.net/" crossorigin="anonymous"></script>
    <script defer src="../public/cards.min%EF%B9%96v=792c672b3c.js"></script>
    <link rel="stylesheet" type="text/css" href="../public/cards.min%EF%B9%96v=792c672b3c.css">
    <!-- link href="https://fonts.googleapis.com/css?family=Noto+Serif+SC&display=swap" rel="stylesheet" -->

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Fira+Mono&amp;family=Noto+Serif+SC&amp;display=swap" rel="stylesheet">

<style>.post-content,.post-card-excerpt{font-family: 'Noto Serif SC', "PingFang SC","Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei","微软雅黑",Arial,sans-serif;}
    .post-full-content{font-size: 100%;}
    .post-full-custom-excerpt {font-size: 1.8rem;}
    .post-full-title {font-size:3.2rem;}
    .post-full-content blockquote{margin:20px;padding: 1em;  background-color: #3eb0ef14; }
    .post-full-content blockquote p {font-style:normal;}
    .post-full-image {display:none;}
    
    /* .post-full-content figcaption {margin: .4em 0 .5em  !important} */
    
    .kg-callout-card {width: 100%; margin-bottom: 1em;}
    
    .post-full-content figure {margin: 0.8em 0 1em;}
    
    p {margin: 0.2em 0 0.5em !important;}
    
    .post-full-content img{border-radius:5px;}
    
    code {font-family: 'Noto Serif SC';}
</style>


<style>:root {--ghost-accent-color: #15171A;}</style>

</head>
<body class="post-template tag-machine-learning tag-algorithm">

    <div class="site-wrapper">

        

<header class="site-header">
    <div class="outer site-nav-main">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left-wrapper">
        <div class="site-nav-left">
                <a class="site-nav-logo" href="../index.html">Pion1eer</a>
            <div class="site-nav-content">
                    <ul class="nav">
    <li class="nav-home"><a href="../index.html">Home</a></li>
    <li class="nav-about"><a href="../about/index.html">About</a></li>
</ul>

                    <span class="nav-post-title dash">LeNet：第一个卷积神经网络</span>
            </div>
        </div>
    </div>
    <div class="site-nav-right">
            <div class="social-links">
            </div>
                <a class="rss-button" href="https://feedly.com/i/subscription/feed/https://www.ruanx.net/rss/" title="RSS" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><circle cx="6.18" cy="17.82" r="2.18"/><path d="M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"/></svg>
</a>

    </div>
</nav>
    </div>
</div>

<script>
 MathJax = {
        tex:{
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            },
        svg:{
                fontCache: 'global'
            }
        }; 
    </script>
    <script src="https://cdn.staticfile.org/babel-polyfill/7.12.1/polyfill.min.js?features=es6"></script>
    <script type="text/javascript" id="MathJax-script" async  src="https://cdn.staticfile.org/mathjax/3.2.2/es5/tex-mml-chtml.js"></script>

<style>@media (prefers-color-scheme: light) {
		.post-full-content  
    pre{background:#fafafa;margin:10px 10px 20px 10px;border-style: solid;border-color:#DCDFE6;}
}
</style>

<link rel="stylesheet" href="https://cdn.ruanx.net/css/atom-one-dark.css">
<link rel="stylesheet" media="(prefers-color-scheme: light)" href="https://cdn.ruanx.net/css/atom-one-light.min.css">
<style>.post-full-content  code{font-family: 'Fira Mono', 'Jetbrains mono', 'ubuntu mono', consolas, 'monospace' !important;}
</style>

</header>


<main id="site-main" class="site-main outer">
    <div class="inner">

        <article class="post-full post tag-machine-learning tag-algorithm no-image no-image">

            <header class="post-full-header">

                <section class="post-full-tags">
                    <a href="../tag/machine-learning/index.html">machine learning</a>
                </section>

                <h1 class="post-full-title">LeNet：第一个卷积神经网络</h1>

                <p class="post-full-custom-excerpt">LeNet 是 Yann LeCun 等人在上世纪 90 年代提出的网络模型，用于手写数字的识别。本文介绍了该模型，并提供 PyTorch 实现。</p>

                <div class="post-full-byline">

                    <section class="post-full-byline-content">

                        <ul class="author-list">
                            <li class="author-list-item">

                                <div class="author-card">
                                    <img class="author-profile-image" src="../content/images/size/w100/2020/05/blue.jpeg" alt="Ruan Xingzhi" />
                                    <div class="author-info">
                                        <div class="bio">
                                            <h2>Ruan Xingzhi</h2>
                                            <p>Welcome to my site and hope you have fun.</p>
                                            <p><a href="../author/blue/index.html">More posts</a> by Ruan Xingzhi.</p>
                                        </div>
                                    </div>
                                </div>

                                <a href="../author/blue/index.html" class="author-avatar">
                                    <img class="author-profile-image" src="../content/images/size/w100/2020/05/blue.jpeg" alt="Ruan Xingzhi" />
                                </a>

                            </li>
                        </ul>

                        <section class="post-full-byline-meta">
                            <h4 class="author-name"><a href="../author/blue/index.html">Ruan Xingzhi</a></h4>
                            <div class="byline-meta-content">
                                <time class="byline-meta-date" datetime="2021-02-24">24 Feb 2021</time>
                                <span class="byline-reading-time"><span class="bull">&bull;</span> 11 min read</span>
                            </div>
                        </section>

                    </section>


                </div>
            </header>


            <section class="post-full-content">
                <div class="post-content">
                    <p>　　「识别手写数字」是一个经典的机器学习任务，有著名的 MNIST 数据集。我们曾经利用多层感知机实现了 90+% 的准确率，本文将介绍卷积神经网络 LeNet，主要参考 <a href="https://towardsdatascience.com/understanding-lenet-a-detailed-walkthrough-17833d4bd155">这篇英文博客</a> 的讲解。</p><p>　　LeNet 是几种神经网络的统称，它们是 Yann LeCun 等人在 1990 年代开发的。一般认为，它们是最早的<strong>卷积神经网络</strong>（Convolutional Neural Networks, CNNs）。模型接收灰度图像，并输出其中包含的手写数字。LeNet 包含了以下三个模型：</p><ul><li>LeNet-1：5 层模型，一个简单的 CNN。</li><li>LeNet-4：6 层模型，是 LeNet-1 的改进版本。</li><li>LeNet-5：7 层模型，最著名的版本。</li></ul><p>　　CNN 的设计是为了模拟人眼的感知方式。传统的 CNN 一般包含以下三种层：</p><ul><li>卷积层（convolution layers）</li><li>降采样层（subsampling layers），或称池化层（pooling layers）</li><li>全连接层（fully connected layers）</li></ul><p>　　它们有各自的用途，通过排列这些层，我们能实现各种 CNN。接下来，我们分别介绍这三种部件。</p><h3 id="-">卷积层</h3><p>　　卷积层利用卷积核（kernel，或称 filter）对图像进行卷积运算。每个 kernel 都是可训练的，有些 kernel 还有 bias 参数。执行卷积运算时，把 kernel 在原图上移动，步长为指定的参数 stride，有时还会做 padding。对于原图内容，将其与 kernel 先进行元素乘法，把结果求和，再加上 bias，就得到输出。</p><p>　　把原图跑完一遍之后，我们在各个位置的卷积结果，生成了特征图（feature map）。显然，kernel 越大，步长越长，padding 越小，则特征图越小。具体而言，满足：$$\text{output size} =\frac{\text{input size} - \text{kernel size} + 2\times \text{padding}}{\text{stride}} + 1$$</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="../content/images/2021/02/1.png" class="kg-image" alt loading="lazy" width="1152" height="576" srcset="../content/images/size/w600/2021/02/1.png 600w, ../content/images/size/w1000/2021/02/1.png 1000w, ../content/images/2021/02/1.png 1152w" sizes="(min-width: 720px) 720px"><figcaption>卷积运算。原图和 kernel 均有 3 个通道。图片来源：<a href="https://towardsdatascience.com/understanding-lenet-a-detailed-walkthrough-17833d4bd155">Azel Daniel</a>，下同。</figcaption></figure><p>　　上图展示了一个典型的二维卷积。原图是 3 通道的，不填充，步长为 0。最终将生成 4×4 的 3 通道特征图。</p><h3 id="--1">池化层</h3><p>　　池化层一般是不用训练的。它的目的是对特征进行降取样，显著减少特征的个数来方便学习。一般有两种池化方式：均值池化（average pooling）和最大池化（max pooling）。前者计算出一个区域的均值，后者从区域中取最大值。</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="../content/images/2021/02/2.png" class="kg-image" alt loading="lazy" width="1152" height="576" srcset="../content/images/size/w600/2021/02/2.png 600w, ../content/images/size/w1000/2021/02/2.png 1000w, ../content/images/2021/02/2.png 1152w" sizes="(min-width: 720px) 720px"><figcaption>池化过程。</figcaption></figure><p>　　上图展示了一个不填充、步长为 2 的池化。6×6×3 的原图在池化之后，变成了 3×3×3 的大小。</p><h3 id="--2">全连接层</h3><p>　　全连接层一般用于 CNN 的最后几层，负责提取卷积和池化之后的特征。这里不过多介绍。接下来，我们开始讨论几种 LeNet。</p><hr><h3 id="lenet-1">LeNet-1</h3><p>　　LeNet-1 仅有五个层，结构如下：</p><figure class="kg-card kg-image-card"><img src="../content/images/2021/02/image-5.png" class="kg-image" alt loading="lazy" width="1728" height="864" srcset="../content/images/size/w600/2021/02/image-5.png 600w, ../content/images/size/w1000/2021/02/image-5.png 1000w, ../content/images/size/w1600/2021/02/image-5.png 1600w, ../content/images/2021/02/image-5.png 1728w" sizes="(min-width: 720px) 720px"></figure><p>　　原先的 28×28 的灰度图像，先经过一个卷积层，生成 4 通道的 feature map；再进行池化降维，然后通过卷积层，得到 12 通道的 feature map。再次池化之后，用全连接层提取特征。具体参数如下：</p><ul><li>C1：卷积层，num_kernels=4, kernel_size=5×5, padding=0, stride=1</li><li>S2：均值池化层，kernel_size=2×2, padding=0, stride=2</li><li>C3：卷积层，num_kernels=12, kernel_size=5×5, padding=0, stride=1</li><li>S4：均值池化层，kernel_size=2×2, padding=0, stride=2</li><li>F5：全连接层，out_features=10</li></ul><p>　　我们需要解释一下 C3 如何用 12 个 kernel，从 4 通道的图中提取出 12 个通道的特征。C3 的每个输出层，是与指定的一些 S2 层相连的，对应的 kernel 长宽是 5×5×n，其中 n 是它所连接的 S2 层数。连接分组方式是超参数（当然我们经常懒得去搞这件事，往往令每个输出层与所有输入层相连，下面的代码也是这样处理的）。</p><figure class="kg-card kg-code-card"><pre><code class="language-python">import torch
import torchvision
import torch.nn as nn
import torchvision.transforms as transforms
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt

trans_to_tensor = transforms.Compose([
    transforms.ToTensor()
])

data_train = torchvision.datasets.MNIST(
    './data', 
    train=True, 
    transform=trans_to_tensor, 
    download=True)

data_test = torchvision.datasets.MNIST(
    './data', 
    train=False, 
    transform=trans_to_tensor, 
    download=True)

data_train, data_test

'''
(Dataset MNIST
     Number of datapoints: 60000
     Root location: ./data
     Split: Train
     StandardTransform
 Transform: Compose(
                ToTensor()
            ),
 Dataset MNIST
     Number of datapoints: 10000
     Root location: ./data
     Split: Test
     StandardTransform
 Transform: Compose(
                ToTensor()
            ))
'''</code></pre><figcaption>▲&nbsp;加载数据</figcaption></figure><p>　　我们随便展示一个数据：</p><pre><code class="language-python">train_loader = torch.utils.data.DataLoader(data_train, batch_size=100, shuffle=True)

x, y = next(iter(train_loader))

plt.imshow(x[0].squeeze(0), cmap='gray'), y[0]</code></pre><figure class="kg-card kg-image-card"><img src="../content/images/2021/02/image-6.png" class="kg-image" alt loading="lazy" width="897" height="355" srcset="../content/images/size/w600/2021/02/image-6.png 600w, ../content/images/2021/02/image-6.png 897w" sizes="(min-width: 720px) 720px"></figure><p>　　训练网络：</p><pre><code class="language-python">def test(net):
    net.eval()
    
    test_loader = torch.utils.data.DataLoader(data_train, batch_size=10000, shuffle=False)
    test_data = next(iter(test_loader))
    
    with torch.no_grad():
        x, y = test_data[0], test_data[1]
    
        outputs = net(x)

        pred = torch.max(outputs, 1)[1]
        print(f'test acc: {sum(pred == y) / outputs.shape[0]}')
    
    net.train()
    
def fit(net, epoch=1):
    net.train()
    run_loss = 0
    
    for num_epoch in range(epoch):
        print(f'epoch {num_epoch}')
        
        for i, data in enumerate(train_loader):
            x, y = data[0], data[1]

            outputs = net(x)
            loss = criterion(outputs, y)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            run_loss += loss.item()

            if i % 100 == 99:
                print(f'[{(i+1) * 100} / 60000] loss={run_loss / 100}')
                run_loss = 0
                
                test(net)</code></pre><p>　　上面是训练 MNIST 的通用框架。接下来我们实现 LeNet-1：</p><pre><code class="language-python">class LeNet1(nn.Module):
    
    def __init__(self):
        super().__init__()
        
        self.conv1 = nn.Conv2d(1, 4, [5, 5])
        self.pool1 = nn.AvgPool2d([2, 2])
        self.conv2 = nn.Conv2d(4, 12, [5, 5])
        self.pool2 = nn.AvgPool2d([2, 2])
        self.fc1 = nn.Linear(12 * 4 * 4, 10)
    
    def forward(self, x):
        x = torch.tanh(self.conv1(x))
        x = self.pool1(x)
        x = torch.tanh(self.conv2(x))
        x = self.pool2(x)
        x = x.view(-1, 12 * 4 * 4)
        x = self.fc1(x)
        
        return x

net_1 = LeNet1()

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(net_1.parameters())

fit(net_1, epoch=5)

'''
epoch 0
[10000 / 60000] loss=1.3996468645334244
test acc: 0.8370000123977661
[20000 / 60000] loss=0.525577874481678
test acc: 0.8773999810218811
[30000 / 60000] loss=0.40644764140248296
test acc: 0.8932999968528748
[40000 / 60000] loss=0.367893455773592
test acc: 0.9024999737739563
[50000 / 60000] loss=0.3431669014692307
test acc: 0.9133999943733215
[60000 / 60000] loss=0.30854775562882425
test acc: 0.916100025177002

....

epoch 4
[10000 / 60000] loss=0.1185341552272439
test acc: 0.9692000150680542
[20000 / 60000] loss=0.10541347645223141
test acc: 0.968999981880188
[30000 / 60000] loss=0.107633958440274
test acc: 0.9714000225067139
[40000 / 60000] loss=0.10533566184341908
test acc: 0.9721999764442444
[50000 / 60000] loss=0.10117398623377084
test acc: 0.9725000262260437
[60000 / 60000] loss=0.09958926556631922
test acc: 0.9736999869346619
'''</code></pre><p>　　可见 LeNet-1 的准确度还是很高的，训练 20 个 epoch 之后可以达到 98.90% 的准确率。另外，我没有调超参数，调一调之后还可以涨更多的点，Github 上有 <a href="https://github.com/HangJie720/lenet-1/blob/master/Lenet.ipynb">网友的代码</a> 的 batchsize 是 64，拿了99.30% 准确率。从 Adam 换成 SGD 也许可以再涨一些点。</p><p>　　接下来要问：卷积起到了什么作用？我们知道，卷积可以完成很多操作，例如高斯模糊、边缘提取等，这是因为卷积是对原图中一块相邻区域的运算。传统的多层感知机，对像素位置之间的关联是很弱的，而卷积层输出的 feature map 中，每个值都是由原图中的一个块产生，故考虑了相邻像素之间的关系。</p><p>　　下面展示两张图片经过 conv1 层之后的结果：</p><figure class="kg-card kg-image-card kg-width-wide"><img src="../content/images/2021/02/3-1.png" class="kg-image" alt loading="lazy" width="1150" height="477" srcset="../content/images/size/w600/2021/02/3-1.png 600w, ../content/images/size/w1000/2021/02/3-1.png 1000w, ../content/images/2021/02/3-1.png 1150w"></figure><p>　　可以发现，第一个 kernel 可以很好地提取「左上－右下」的线条；第二、三个 kernel 可以提取横向的线条；第四个 kernel 可以提取「左下－右上」的线条。这不是我们钦定的，而是它们自己学习出来的。我们从中可以体会到 CNN 提取图片「部件」的独到之处。</p><h3 id="lenet-4">LeNet-4</h3><p>　　LeNet-4 是 LeNet-1 的改进版本，有 6 个层。它们主要的不同之处在于，LeNet-4 采用了两个全连接层来提取特征。另外，LeNet-4 的输入是 32×32 的灰度图。</p><figure class="kg-card kg-image-card"><img src="../content/images/2021/02/image-7.png" class="kg-image" alt loading="lazy" width="1587" height="766" srcset="../content/images/size/w600/2021/02/image-7.png 600w, ../content/images/size/w1000/2021/02/image-7.png 1000w, ../content/images/2021/02/image-7.png 1587w" sizes="(min-width: 720px) 720px"></figure><p>　　每个层的参数如下：</p><ul><li>C1：卷积层，num_kernels=4, kernel_size=5×5, padding=0, stride=1</li><li>S2：均值池化层，kernel_size=2×2, padding=0, stride=2</li><li>C3：卷积层，num_kernels=16, kernel_size=5×5, padding=0, stride=1</li><li>S4：均值池化层，kernel_size=2×2, padding=0, stride=2</li><li>F5：全连接层，out_features=120</li><li>F6：全连接层，out_features=10</li></ul><p>　　Yann LeCun 在 USPS 数据集上，测得 LeNet-1 的误判率是 1.7%，有 3246 个参数；LeNet-4 在 MNIST 上的误判率是 1.1%，但有 51050 个参数。像这种架构的 CNN，全连接层占了绝大部分的参数。</p><h3 id="lenet-5">LeNet-5</h3><p>　　LeNet-5 是 LeNet-4 的改进版，即现在我们熟知的那个 LeNet。它的层数达到了 7 层，获取 32×32 的输入，有 60850 个参数。</p><figure class="kg-card kg-image-card"><img src="../content/images/2021/02/image-8.png" class="kg-image" alt loading="lazy" width="1728" height="864" srcset="../content/images/size/w600/2021/02/image-8.png 600w, ../content/images/size/w1000/2021/02/image-8.png 1000w, ../content/images/size/w1600/2021/02/image-8.png 1600w, ../content/images/2021/02/image-8.png 1728w" sizes="(min-width: 720px) 720px"></figure><p>　　在 MNIST 数据集上，Yann LeCun 的 LeNet-5 具有 0.95% 的低误判率。其各层参数如下：</p><ul><li>C1：卷积层，num_kernels=6, kernel_size=5×5, padding=0, stride=1</li><li>S2：均值池化层，kernel_size=2×2, padding=0, stride=2</li><li>C3：卷积层，num_kernels=16, kernel_size=5×5, padding=0, stride=1</li><li>S4：均值池化层，kernel_size=2×2, padding=0, stride=2</li><li>F5：全连接层，out_features=140</li><li>F6：全连接层，out_features=84</li><li>F7：全连接层，out_features=10</li></ul><p>　　S2 层有个需要注意的地方：它的每一个输出通道，是由对应的输入通道的池化结果，乘以 weight 再加上 bias。它们都是可训练的，于是 S2 层有了 12 个可训练参数（共 6 个通道，每个通道训练自己的 weight 和 bias）。S4 层与之类似，有 32 个可训练参数。</p><p>　　C3 的每个输出通道，只与 S2 指定的若干个通道相连。具体的分组方法如下：</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="../content/images/2021/02/image-9.png" class="kg-image" alt loading="lazy" width="570" height="201"><figcaption>每一列是一个输出通道，有「X」的行表示与之相连的 S2&nbsp;通道</figcaption></figure><p>　　我们在代码里面继续偷懒，让 C3 的每个输出通道与 S2 所有通道相连。另外，由于 LeNet-5 输入为 32×32，而我们手上的 MNIST 是 28×28 的，于是做 2 像素的 padding，来形成 32×32 的图。</p><pre><code class="language-python">class LeNet5(nn.Module):
    
    def __init__(self):
        super().__init__()
        
        self.conv1 = nn.Conv2d(1, 6, 5, padding=2)
        self.pool1 = nn.AvgPool2d(2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.pool2 = nn.AvgPool2d(2)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = torch.tanh(self.conv1(x))
        x = self.pool1(x)
        x = torch.tanh(self.conv2(x))
        x = self.pool2(x)
        
        x = x.view(-1, 16 * 5 * 5)
        x = self.fc1(x)
        x = self.fc2(x)
        x = self.fc3(x)
        
        return x

net_5 = LeNet5()

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(net_5.parameters())

fit(net_5, epoch=20)

'''
epoch 0
[10000 / 60000] loss=0.8946414443850518
test acc: 0.8745999932289124
[20000 / 60000] loss=0.3811839409172535
test acc: 0.9031000137329102
[30000 / 60000] loss=0.34581568479537966
test acc: 0.9114999771118164
[40000 / 60000] loss=0.299122948423028
test acc: 0.9218000173568726
[50000 / 60000] loss=0.25059412196278574
test acc: 0.927299976348877
[60000 / 60000] loss=0.22540301881730557
test acc: 0.9373000264167786

...

epoch 19
[10000 / 60000] loss=0.024130959606263786
test acc: 0.9918000102043152
[20000 / 60000] loss=0.02669174194626976
test acc: 0.9901000261306763
[30000 / 60000] loss=0.02360637279227376
test acc: 0.9883000254631042
[40000 / 60000] loss=0.023022194614750333
test acc: 0.9933000206947327
[50000 / 60000] loss=0.035112172679509966
test acc: 0.9896000027656555
[60000 / 60000] loss=0.03023288542899536
test acc: 0.9898999929428101
'''</code></pre><p>　　注意到 LeNet-5 的泛化能力确实比 LeNet-1 好了一些，这边迭代 20 次之后，可以涨到 99.33% 的准确率。</p><p>　　来看 conv1 处理之后的结果：</p><figure class="kg-card kg-image-card kg-width-wide"><img src="../content/images/2021/02/4.png" class="kg-image" alt loading="lazy" width="1150" height="391" srcset="../content/images/size/w600/2021/02/4.png 600w, ../content/images/size/w1000/2021/02/4.png 1000w, ../content/images/2021/02/4.png 1150w"></figure><p>　　这里我们可以看出，kernel 3、5 可以提取上边缘，kernel 4 可以提取下边缘。kernel 1 可以提取竖线，kernel 2 可以提取横线。</p><hr><p>　　Yann LeCun 训练这些 LeNet 的时候，对数据进行了预处理，使之均值为 0，方差大概为 1。另外，当时采用的是 MSE loss，本文采用了交叉熵。LeNet 是一个划时代的模型，但它也有一些不足之处：</p><ol><li>网络很小，限制了应用场景。</li><li>LeNet 采用了均值池化，但我们现在倾向于采用最大池化，这样可以加速收敛。</li><li>LeNet 采用的激活函数是 tanh，但我们现在倾向于使用 ReLU。实践上，ReLU 往往能够有更好的正确率。</li></ol><p>　　总结一句，LeNet 给我们提供的思路是：用卷积提取与位置相关的信息；用池化来减少特征数量；用全连接来提取特征、进行预测。</p>
                </div>
            </section>


            <div id="gitalk-container"></div>

            <link rel="stylesheet" href="https://cdn.staticfile.org/gitalk/1.6.2/gitalk.css">
            <script src="https://cdn.staticfile.org/gitalk/1.6.2/gitalk.min.js"></script>

            <script>
                const gitalk = new Gitalk({

                    proxy: 'https://cors.pion1eer.workers.dev/?https://github.com/login/oauth/access_token',

                    clientID: '9f7e67174f5ab8b1a2b9',
                    clientSecret: 'ddee425a0b50ed02a05fdedb1a4ea039e01b3170',
                    repo: 'blogComment',
                    owner: 'Ruanxingzhi',
                    admin: ['Ruanxingzhi'],
                    id: location.pathname,      // Ensure uniqueness and length less than 50
                    distractionFreeMode: false  // Facebook-like distraction free mode

                    });

                gitalk.render('gitalk-container');
            </script>

        </article>

    </div>
</main>

<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
                <article class="read-next-card">
                    <header class="read-next-card-header">
                        <h3><span>More in</span> <a href="../tag/machine-learning/index.html">machine learning</a></h3>
                    </header>
                    <div class="read-next-card-content">
                        <ul>
                            <li>
                                <h4><a href="../training-gan/index.html">训练一个 GAN</a></h4>
                                <div class="read-next-card-meta">
                                    <p><time datetime="2021-02-28">28 Feb 2021</time> –
                                        10 min read</p>
                                </div>
                            </li>
                        </ul>
                    </div>
                    <footer class="read-next-card-footer">
                        <a href="../tag/machine-learning/index.html">1 post
                            →</a>
                    </footer>
                </article>

                <article class="post-card post tag-machine-learning no-image no-image">


    <div class="post-card-content">

        <a class="post-card-content-link" href="../training-gan/index.html">

            <header class="post-card-header">
                    <div class="post-card-primary-tag">machine learning</div>
                <h2 class="post-card-title">训练一个 GAN</h2>
            </header>

            <section class="post-card-excerpt">
                    <p>本文记录了训练 GAN 来生成二次元头像的过程。我们先从网上获取二次元头像数据集，训练一个生成器、训练一个判别器，来获取生产二次元头像的网络。</p>
            </section>

        </a>

        <footer class="post-card-meta">
            <ul class="author-list">
                <li class="author-list-item">
            
                    <div class="author-name-tooltip">
                        Ruan Xingzhi
                    </div>
            
                    <a href="../author/blue/index.html" class="static-avatar">
                        <img class="author-profile-image" src="../content/images/size/w100/2020/05/blue.jpeg" alt="Ruan Xingzhi" />
                    </a>
                </li>
            </ul>
            <div class="post-card-byline-content">
                <span><a href="../author/blue/index.html">Ruan Xingzhi</a></span>
                <span class="post-card-byline-date"><time datetime="2021-02-28">28 Feb 2021</time> <span class="bull">&bull;</span> 10 min read</span>
            </div>
        </footer>

    </div>

</article>

                <article class="post-card post tag-algorithm no-image no-image">


    <div class="post-card-content">

        <a class="post-card-content-link" href="../cheat-neural-network/index.html">

            <header class="post-card-header">
                    <div class="post-card-primary-tag">algorithm</div>
                <h2 class="post-card-title">欺骗神经网络</h2>
            </header>

            <section class="post-card-excerpt">
                    <p>对神经网络的输入施加微小扰动，可能大幅度改变其输出。在本文中，我们训练了一个针对 MNIST 的简单网络，并小幅度修改指定的图片，让它作出错误判断。</p>
            </section>

        </a>

        <footer class="post-card-meta">
            <ul class="author-list">
                <li class="author-list-item">
            
                    <div class="author-name-tooltip">
                        Ruan Xingzhi
                    </div>
            
                    <a href="../author/blue/index.html" class="static-avatar">
                        <img class="author-profile-image" src="../content/images/size/w100/2020/05/blue.jpeg" alt="Ruan Xingzhi" />
                    </a>
                </li>
            </ul>
            <div class="post-card-byline-content">
                <span><a href="../author/blue/index.html">Ruan Xingzhi</a></span>
                <span class="post-card-byline-date"><time datetime="2021-02-06">6 Feb 2021</time> <span class="bull">&bull;</span> 7 min read</span>
            </div>
        </footer>

    </div>

</article>
        </div>
    </div>
</aside>




        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="../index.html">Pion1eer</a> &copy; 2023</section>
                <nav class="site-footer-nav">
                    <a href="http://beian.miit.gov.cn/" target="_blank">湘ICP备20008720号-1</a>
                    <a href="../index.html">Latest Posts</a>
                    
                    
                    <a href="https://ghost.org" target="_blank" rel="noopener">Ghost</a>
                </nav>
            </div>
        </footer>

    </div>

<!--

    <div class="subscribe-success-message">
        <a class="subscribe-close" href="javascript:;"></a>
        You've successfully subscribed to Pion1eer!
    </div>

    <div id="subscribe" class="subscribe-overlay">
        <a class="subscribe-close" href="#"></a>
        <div class="subscribe-overlay-content">
            <div class="subscribe-form">
                <h1 class="subscribe-overlay-title">Subscribe to Pion1eer</h1>
                <p class="subscribe-overlay-description">Stay up to date! Get all the latest & greatest posts delivered straight to your inbox</p>
                <form data-members-form="subscribe">
                    <div class="form-group">
                        <input class="subscribe-email" data-members-email placeholder="youremail@example.com"
                            autocomplete="false" />
                        <button class="button primary" type="submit">
                            <span class="button-content">Subscribe</span>
                            <span class="button-loader"><svg version="1.1" id="loader-1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px"
    y="0px" width="40px" height="40px" viewBox="0 0 40 40" enable-background="new 0 0 40 40" xml:space="preserve">
    <path opacity="0.2" fill="#000" d="M20.201,5.169c-8.254,0-14.946,6.692-14.946,14.946c0,8.255,6.692,14.946,14.946,14.946
s14.946-6.691,14.946-14.946C35.146,11.861,28.455,5.169,20.201,5.169z M20.201,31.749c-6.425,0-11.634-5.208-11.634-11.634
c0-6.425,5.209-11.634,11.634-11.634c6.425,0,11.633,5.209,11.633,11.634C31.834,26.541,26.626,31.749,20.201,31.749z" />
    <path fill="#000" d="M26.013,10.047l1.654-2.866c-2.198-1.272-4.743-2.012-7.466-2.012h0v3.312h0
C22.32,8.481,24.301,9.057,26.013,10.047z">
        <animateTransform attributeType="xml" attributeName="transform" type="rotate" from="0 20 20" to="360 20 20"
            dur="0.5s" repeatCount="indefinite" />
    </path>
</svg></span>
                        </button>
                    </div>
                    <div class="message-success">
                        <strong>Great!</strong> Check your inbox and click the link to confirm your subscription.
                    </div>
                    <div class="message-error">
                        Please enter a valid email address!
                    </div>
                </form>
            </div>
        </div>
    </div>

-->

    <script
        src="https://cdn.ruanx.net/js/jquery.min.js"
        integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
        crossorigin="anonymous">
    </script>
    <script src="../assets/built/casper%EF%B9%96v=792c672b3c.js"></script>

    <script>
        // Parse the URL parameter
        function getParameterByName(name, url) {
            if (!url) url = window.location.href;
            name = name.replace(/[\[\]]/g, "\\$&");
            var regex = new RegExp("[?&]" + name + "(=([^&#]*)|&|#|$)"),
                results = regex.exec(url);
            if (!results) return null;
            if (!results[2]) return '';
            return decodeURIComponent(results[2].replace(/\+/g, " "));
        }

        // Give the parameter a variable name
        var action = getParameterByName('action');

        $(document).ready(function () {
            if (action == 'subscribe') {
                $('body').addClass("subscribe-success");
            }

            $('.subscribe-success-message .subscribe-close').click(function () {
                $('.subscribe-success-message').addClass('close');
            });

            // Reset form on opening subscrion overlay
            $('.subscribe-button').click(function() {
                $('.subscribe-overlay form').removeClass();
                $('.subscribe-email').val('');
            });
        });
    </script>

    <script>
    $(document).ready(function () {
        // FitVids - start
        var $postContent = $(".post-full-content");
        $postContent.fitVids();
        // FitVids - end

        // Replace nav with title on scroll - start
        Casper.stickyNavTitle({
            navSelector: '.site-nav-main',
            titleSelector: '.post-full-title',
            activeClass: 'nav-post-title-active'
        });
        // Replace nav with title on scroll - end

        // Hover on avatar
        var hoverTimeout;
        $('.author-list-item').hover(function () {
            var $this = $(this);

            clearTimeout(hoverTimeout);

            $('.author-card').removeClass('hovered');
            $(this).children('.author-card').addClass('hovered');

        }, function () {
            var $this = $(this);

            hoverTimeout = setTimeout(function () {
                $this.children('.author-card').removeClass('hovered');
            }, 800);
        });
    });
</script>

<script src="https://cdn.staticfile.org/highlight.js/10.1.1/highlight.min.js"></script>
<script >hljs.initHighlightingOnLoad();</script>



    <style>.kg-bookmark-metadata > .kg-bookmark-publisher:before {
    content: "" !important; 
    margin: 0 !important;
}
</style>

</body>
</html>
